# **MegaTruth ğŸ”**

<div align="center">
  <img src="images/logo/logo_mega_truth.png" alt="Logo MegaTruth" width="250"/>
  <br>
  <b>Sistema Forense Multimodal para DetecÃ§Ã£o e ExplicaÃ§Ã£o de Imagens Geradas por IA</b>
</div>

---

O **MegaTruth** Ã© uma ferramenta avanÃ§ada de perÃ­cia digital que utiliza uma arquitetura de **Ensemble HÃ­brido** para detectar anomalias, localizar inconsistÃªncias visuais (como mÃ£os deformadas e falhas de iluminaÃ§Ã£o) e gerar laudos tÃ©cnicos em linguagem natural.

Diferente de detectores simples que apenas dizem "Real" ou "Fake", o MegaTruth explica o **porquÃª**.

---

## **ğŸ§  Arquitetura do Sistema**

O sistema opera simulando o fluxo de trabalho de uma equipe forense:

### **1. O Juiz (CLIP Fine-Tuned)**
* **Local:** `src/models/clip_finetuned`
* **FunÃ§Ã£o:** ClassificaÃ§Ã£o de Alto NÃ­vel. Analisa texturas invisÃ­veis e ruÃ­do de compressÃ£o.

### **2. O Analista SemÃ¢ntico (CLIP Base + Configs)**
* **Local:** `src/models/config/concepts.txt`
* **FunÃ§Ã£o:** *Concept Bottleneck*. Se a imagem Ã© suspeita, este modelo varre uma lista de **80+ conceitos forenses** (ex: "mÃ£os deformadas", "fÃ­sica impossÃ­vel").

### **3. O Artista (CLIPSeg)**
* **Local:** `src/models/vision_model_clip.py`
* **FunÃ§Ã£o:** SegmentaÃ§Ã£o SemÃ¢ntica. Recebe o defeito encontrado (ex: "olhos assimÃ©tricos") e gera uma **MÃ¡scara de EvidÃªncia** precisa sobre a regiÃ£o, em vez de um mapa de calor genÃ©rico.

### **4. O Perito (IA Generativa)**
* **Local:** `src/models/multimodal_model_*.py`
* **Tecnologia:** HÃ­brida. Usa **Nemotron-12B** (Via API/Nuvem) para mÃ¡xima inteligÃªncia ou **LLaVA** (Local,via ollama) como fallback.
* **SaÃ­da:** Um laudo textual detalhado cruzando os dados visuais com o contexto da imagem.

---

## **ğŸ“‚ Estrutura do Projeto**

```bash
MegaTruth/
â”‚
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ experiment              # Pasta contendo imagens usadas em alguns experimentos
â”‚   â”‚   â”œâ”€â”€ erros               # Imagens que deram Falso Positivo ou Falso Negativo
â”‚   â”‚   â”œâ”€â”€ IA                  # Imagens feitas por IA que foram detectadas corretamente
â”‚   â”‚   â””â”€â”€ real                # Imagens Reais que foram detectadas corretamente 
â”‚   â”œâ”€â”€ inferences/             # Dataset de validaÃ§Ã£o e testes
â”‚   â”‚   â”œâ”€â”€ AI/                 # (+250 imagens geradas por Midjourney, Flux, etc.)
â”‚   â”‚   â””â”€â”€ real/               # (+250 fotografias reais de controle)
â”‚   â”œâ”€â”€ logo/                   # Assets visuais do projeto
â”‚   â””â”€â”€ uploaded/               # Imagens enviadas pelos usuÃ¡rios via Interface
â”‚
â”œâ”€â”€ notebooks/                  # Pesquisa e Desenvolvimento 
â”‚   â”œâ”€â”€ clip-finetunning.ipynb          # Fine-tuning do classificador especialista (clip)
â”‚   â””â”€â”€ concept_bottleneck_eval.ipynb   # ValidaÃ§Ã£o da sensibilidade semÃ¢ntica (Base vs Tuned)
â”‚
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ evidence_masks/         # MÃ¡scaras de evidÃªncia geradas temporariamente
â”‚   â””â”€â”€ reports/                # Logs e transcriÃ§Ãµes de execuÃ§Ã£o
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/                 # O "CÃ©rebro" do sistema
â”‚   â”‚   â”œâ”€â”€ clip_finetuned/     # pasta contendo arquivos do clip fine-tuned
â”‚   â”‚   â”œâ”€â”€ config/             # ConfiguraÃ§Ãµes de conhecimento
â”‚   â”‚   â”‚   â”œâ”€â”€ anchors.txt     # Mapeamento Conceito -> Objeto Visual para o clip no defect_map
â”‚   â”‚   â”‚   â””â”€â”€ concepts.txt    # Lista de defeitos de IA conhecidos para o clip no concept bottleneck
â”‚   â”‚   â”œâ”€â”€ vision_model_clip.py           # modelo de visÃ£o (classificaÃ§Ã£o + concept bottleneck + mapa de calor)
â”‚   â”‚   â”œâ”€â”€ multimodal_model_llava.py      # modelo multimodal local (ExplicaÃ§Ã£o da classificaÃ§Ã£o)
â”‚   â”‚   â””â”€â”€ multimodal_model_nemotron.py   # modelo multimodal Nuvem (via API da Open Router - explicaÃ§Ã£o da classificaÃ§Ã£o)
â”‚   â”‚
â”‚   â”œâ”€â”€ test/                   # Scripts de Teste e Debug dos modelos multimodais
â”‚   â”‚   â”œâ”€â”€ main_app_llava_test.py
â”‚   â”‚   â””â”€â”€ main_app_nemotron_test.py
â”‚   â”‚
â”‚   â””â”€â”€ ui/                     # Frontend
â”‚       â””â”€â”€ gradio_app.py       # Interface Web Principal
â”‚
â””â”€â”€ requirements.txt
```

## **ğŸš€ InstalaÃ§Ã£o e ExecuÃ§Ã£o**

### **1. PrÃ©-requisitos**

- Python 3.10 ou superior.
- Placa de vÃ­deo NVIDIA (recomendada para performance com CUDA).
- Ollama instalado (se for usar o modo Local/Offline)
> ğŸ”— DisponÃ­vel em: **https://ollama.com/download**

### **2. InstalaÃ§Ã£o das dependÃªncias**

Abra um terminal na pasta raiz do projeto e execute:

```bash
# 1. Crie um ambiente virtual (recomendado)
python -m venv venv

# 2. Ative o ambiente (PowerShell)
venv\Scripts\Activate.ps1

# 3. Instale os pacotes necessÃ¡rios
pip install -r requirements.txt
```

**ObservaÃ§Ã£o**: no `cmd.exe` a ativaÃ§Ã£o Ã© `venv\Scripts\activate` e no `bash`/`zsh` (Linux/Mac) Ã© `source venv/bin/activate`.

### **3. ConfiguraÃ§Ã£o (opcional â€” modelo multimodal Via API)**

O sistema funciona offline com o LLaVA, mas para laudos periciais mais consistentes e de alta qualidade recomenda-se usar o Nemotron via OpenRouter.

Abaixo estÃ£o as instruÃ§Ãµes completas para criar sua chave e configurar o ambiente.

---
### **ğŸŒ 1. Criar uma conta no OpenRouter**

Acesse o site oficial: ğŸ”— https://openrouter.ai

Para criar sua conta:

1. Clique em **Sign Up** no canto superior direito.
2. Escolha como deseja se registrar:
   - Google  
   - GitHub  
   - E-mail + senha
3. ApÃ³s o cadastro, confirme seu e-mail (caso solicitado).
4. FaÃ§a login normalmente acessando **Sign In**.
5. Depois de logado, vocÃª serÃ¡ direcionado ao painel principal (**Dashboard**).

A partir daÃ­, vocÃª jÃ¡ pode criar sua API Key.

---

### ğŸ”‘ 2. Como criar sua API Key no OpenRouter

1. Entre no site: https://openrouter.ai  
2. Clique em **Sign Up** (se ainda nÃ£o tiver conta) ou **Sign In** (para entrar).  
3. ApÃ³s logar, vÃ¡ atÃ© a pÃ¡gina:  
   **API Keys â†’ https://openrouter.ai/settings/keys**
4. Clique em **Create Key**.  
5. Escolha um nome para a chave (ex.: `nemotron-producao`)  
6. Copie a chave gerada **imediatamente** â€” ela sÃ³ aparece uma vez.  
7. Guarde em local seguro e nÃ£o compartilhe com ninguÃ©m.

---

### ğŸ—‚ï¸ 3. Configurar o arquivo `.env`

Crie um arquivo `.env` na raiz do projeto com:

```ini
OPENROUTER_API_KEY="sua-chave-aqui"
```

Se o `.env` nÃ£o existir ou a chave for invÃ¡lida, o sistema farÃ¡ fallback automÃ¡tico para o LLaVA local.

### **4. Download do clip Fine-Tuned**

O GitHub nÃ£o permite versionar arquivos maiores que **100 MB**, por isso o modelo **CLIP Fine-Tuned** nÃ£o estÃ¡ incluÃ­do diretamente no repositÃ³rio.

Para obter uma melhor precisÃ£o na classificaÃ§Ã£o das imagens, baixe o modelo Fine-Tuned manualmente pelo link abaixo:

ğŸ”— DisponÃ­vel em: **https://drive.google.com/drive/folders/1kwe6CK709BzBrYZ7miaHf2G9k1N_dWBs?usp=sharing**

ApÃ³s o download:

1. Crie uma pasta nomeada de **clip_finetuned** no seguinte caminho do projeto:

```bash
src/models/clip_finetuned
```

**2. Extraia o conteÃºdo do arquivo `.zip` dentro dessa pasta.**

A estrutura final deve ficar assim:

```bash
src/
â””â”€â”€ models/                
       â””â”€â”€ clip_finetuned/           <-------- PASTA DO CLIP FINE-TUNED
                â”œâ”€â”€ config.json
                â”œâ”€â”€ merges.txt
                â”œâ”€â”€ model.safetensors
                â”œâ”€â”€ preprocessor_config.json
                â”œâ”€â”€ special_tokens_map.json
                â”œâ”€â”€ tokenizer_config.json
                â”œâ”€â”€ tokenizer.json
                â””â”€â”€ vocab.json
       â”œâ”€â”€ config/             
       â”œâ”€â”€ vision_model_clip.py        
       â”œâ”€â”€ multimodal_model_llava.py  
       â””â”€â”€ multimodal_model_nemotron.py
```

### **5. Executando a interface (Gradio)**

Inicie a interface com:

```bash
python src/ui/gradio_app.py
```

ApÃ³s alguns segundos o terminal exibirÃ¡ um link local (ex: `http://127.0.0.1:7860`). Segure a tecla **ctrl** e clique com o botÃ£o esquerdo no `http://127.0.0.1:7860` para Abrir no navegador.

## **ğŸ§ª Pesquisa & ValidaÃ§Ã£o**

O projeto inclui notebooks que validam a eficÃ¡cia da arquitetura hÃ­brida:

- `notebooks/concept_bottleneck_eval.ipynb`: demonstra que o CLIP Base possui maior sensibilidade para a detecÃ§Ã£o conceitos de imagens gerados por IA, enquanto o Fine-Tuned atua como filtro para reduzir falsos positivos.
- `notebooks/clip-finetunning.ipynb`: documenta o processo de fine-tuning do do modelo em detecÃ§Ã£o de imagens geradas por IA


## ğŸ’¡ Roadmap do MegaTruth (Checklist)

Aqui estÃ¡ o *roadmap* do MegaTruth formatado como uma lista de verificaÃ§Ã£o (checklist), detalhando os subtÃ³picos e entregÃ¡veis para cada mÃ³dulo planejado.

---

### 1. GUI (Gradio/Streamlit)

CriaÃ§Ã£o da interface de usuÃ¡rio **simples e funcional** para demonstraÃ§Ãµes e usabilidade.

* [x] **Design e Estrutura Inicial (MVP):**
    * [x] Definir o *framework* de UI (Gradio/Streamlit).
    * [x] Definir o *framework* de UI (Gradio)
    * [x] Implementar o componente de **Upload de Imagem** (`PNG`, `JPG`).
* [x] **MÃ³dulo de SaÃ­da Principal:**
    * [x] Exibir **RÃ³tulo de ClassificaÃ§Ã£o** (`Real` vs `IA`) e **ConfianÃ§a**.
    * [x] Ãrea dedicada Ã  visualizaÃ§Ã£o do **defect_map** (Grad-CAM).
    * [x] Caixa de texto para a **ExplicaÃ§Ã£o Textual** (saÃ­da do LLaVA).
* [x] **Funcionalidades Adicionais:**
    * [x] Criar um **HistÃ³rico Simples** de anÃ¡lises da sessÃ£o.

---

### 2. Finetuning do CLIP

Melhoria da precisÃ£o e **robustez** do classificador CLIP para o domÃ­nio *real vs IA*.

* [x] **PreparaÃ§Ã£o do Dataset Especializado:**
    * [x] Curadoria de um **dataset balanceado** (Real vs. IA de mÃºltiplos modelos generativos).
    * [x] Implementar **EstratÃ©gia de Aumento de Dados** (*Data Augmentation*) simulando compressÃ£o (JPEG) e ruÃ­do.
* [x] **ImplementaÃ§Ã£o do Finetuning (LoRA):**
    * [x] Selecionar o *backbone* CLIP e definir a **arquitetura LoRA**.
    * [x] Treinar o modelo utilizando LoRA e definir hiperparÃ¢metros (taxa de aprendizado, Ã©pocas).
* [x] **AvaliaÃ§Ã£o e ComparaÃ§Ã£o:**
    * [x] Estabelecer a **linha de base (*baseline*)** do CLIP sem *finetuning*.
    * [x] Avaliar o modelo *finetunado* em mÃ©tricas como **AcurÃ¡cia, AUC e F1-Score**..

---

### 3. Concept Bottleneck (Explicabilidade Profunda)

Fornecer explicaÃ§Ãµes intermediÃ¡rias baseadas em **conceitos semÃ¢nticos e visuais** de artefatos. 

[Image of a Concept Bottleneck Model diagram showing input, concept layer, and output]


* [x] **DefiniÃ§Ã£o de Conceitos:**
    * [x] Definir uma ontologia de **artefatos de IA** e **inconsistÃªncias visuais** (ex: "Dedos Deformados", "Textura Irregular").
    * [ ] Rotular um subconjunto do *dataset* com a **presenÃ§a/ausÃªncia** desses conceitos.
* [x] **ValidaÃ§Ã£o do Extrator de Conceitos:**
    * [x] Avaliar se o CLIP Base (Zero-Shot) possui sensibilidade suficiente para atuar como o Gargalo Conceitual (CBM) sem necessidade de treino adicional.
* [x] **IntegraÃ§Ã£o ao LLaVA:**
    * [x] Modificar o *prompt* do LLaVA para incluir a **Lista de Conceitos Preditos**.
    * [x] Instruir o LLaVA a **incorporar esses conceitos** na explicaÃ§Ã£o textual.
