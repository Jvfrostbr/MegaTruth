{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4678cad",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:32px; font-weight:700; margin-bottom:10px;\"> üß™ Avalia√ß√£o T√©cnica: Concept Bottleneck (Base vs. Tuned)</h1>\n",
    "\n",
    "<h2 style=\"font-size:24px; margin-top:20px;\"> üéØ Novo Objetivo</h2>\n",
    "<p style=\"font-size:16px; line-height:1.6;\">\n",
    "Investigar a hip√≥tese de <b>perda de sensibilidade sem√¢ntica</b>.\n",
    "<br>O objetivo deste notebook √© analisar se o processo de Fine-Tuning (focado em texturas/artefatos) tornou o modelo <b>menos capaz</b> de identificar conceitos visuais complexos (<i>Concept Bottleneck</i>) em compara√ß√£o ao modelo <b>CLIP Base (OpenAI)</b>.\n",
    "<br><br>\n",
    "Esperamos confirmar que o <b>Modelo Base</b> atua melhor como \"Generalista\" (detectando anatomia e objetos), enquanto o <b>Fine-Tuned</b> atua melhor como \"Especialista\" (detectando ru√≠do invis√≠vel), justificando o uso de uma arquitetura h√≠brida.\n",
    "</p>\n",
    "\n",
    "<h2 style=\"font-size:22px; margin-top:20px;\"> üìÇ Entrada</h2>\n",
    "<ul>\n",
    "    <li><code>images/real/</code> (Dataset de Controle - Fotos Reais)</li>\n",
    "    <li><code>images/AI/</code> (Dataset de Teste - Imagens Geradas)</li>\n",
    "    <li>Mega Lista de 60+ Conceitos de Artefatos (Anatomia, F√≠sica, L√≥gica)</li>\n",
    "</ul>\n",
    "\n",
    "<h2 style=\"font-size:22px; margin-top:20px;\"> üìä Sa√≠da Esperada</h2>\n",
    "<p style=\"font-size:16px; line-height:1.6;\">\n",
    "Um <b>Comparativo de Sensibilidade</b> demonstrando:\n",
    "<br>1. <b>Volume de Detec√ß√µes:</b> Quantos conceitos cada modelo ativa em m√©dia por imagem? (Hip√≥tese: Base >> Tuned).\n",
    "<br>2. <b>Sobreposi√ß√£o:</b> O modelo Tuned ainda enxerga os mesmos defeitos √≥bvios (ex: m√£os deformadas) que o Base?\n",
    "</p>\n",
    "\n",
    "<hr style=\"margin:20px 0;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f47754",
   "metadata": {},
   "source": [
    "### **Imports e configura√ß√£o**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964e7c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiente configurado. Usando dispositivo: cuda\n",
      "Raiz do Projeto: c:\\Users\\joaov\\OneDrive\\Documentos\\GitHub\\MegaTruth\n",
      "Buscando imagens em:\n",
      "   - c:\\Users\\joaov\\OneDrive\\Documentos\\GitHub\\MegaTruth\\images\\inferences\\real\n",
      "   - c:\\Users\\joaov\\OneDrive\\Documentos\\GitHub\\MegaTruth\\images\\inferences\\AI\n",
      "Modelo Tuned esperado em: c:\\Users\\joaov\\OneDrive\\Documentos\\GitHub\\MegaTruth\\src\\models\\clip_finetuned\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. Pega o diret√≥rio onde o notebook est√° rodando\n",
    "notebook_dir = os.getcwd()\n",
    "\n",
    "# 2. Sobe um n√≠vel (..) para achar a Raiz do Projeto\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "\n",
    "# 3. Adiciona 'src' ao path do sistema (partindo da raiz) para os imports funcionarem\n",
    "sys.path.append(os.path.join(project_root, 'src'))\n",
    "\n",
    "# 4. Caminhos dos Dados (Agora usando project_root como base)\n",
    "PATH_REAL = os.path.join(project_root, \"images\", \"inferences\", \"real\")\n",
    "PATH_AI = os.path.join(project_root, \"images\", \"inferences\", \"AI\")\n",
    "PATH_MODEL_TUNED = os.path.join(project_root, \"src\", \"models\", \"clip_finetuned\")\n",
    "\n",
    "# Configura√ß√£o de Hardware\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Ambiente configurado. Usando dispositivo: {DEVICE}\")\n",
    "print(f\"Raiz do Projeto: {project_root}\")\n",
    "print(\"Buscando imagens em:\")\n",
    "print(f\"   - {PATH_REAL}\")\n",
    "print(f\"   - {PATH_AI}\")\n",
    "print(f\"Modelo Tuned esperado em: {PATH_MODEL_TUNED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7bd7bf",
   "metadata": {},
   "source": [
    "### **Fun√ß√µes auxiliares**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0db4893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def carregar_imagens(pasta, limite=250):\n",
    "    \"\"\"Carrega caminhos de imagens jpg/png da pasta.\"\"\"\n",
    "    if not os.path.exists(pasta):\n",
    "        print(f\"Pasta n√£o encontrada: {pasta}\")\n",
    "        return []\n",
    "    \n",
    "    arquivos = []\n",
    "    for ext in [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.webp\"]:\n",
    "        arquivos.extend(glob.glob(os.path.join(pasta, ext)))\n",
    "    \n",
    "    print(f\"   -> Encontradas {len(arquivos)} imagens em '{os.path.basename(pasta)}'.\")\n",
    "    return arquivos[:limite]\n",
    "\n",
    "def rodar_teste(model_instance, lista_imagens, nome_teste):\n",
    "    \"\"\"\n",
    "    Roda a detec√ß√£o de conceitos em um lote de imagens.\n",
    "    Aplica a l√≥gica de Gating simulado (Real=0, Fake=1).\n",
    "    \"\"\"\n",
    "    contagem_conceitos = []\n",
    "    print(f\"‚ö° Processando {nome_teste}...\")\n",
    "    \n",
    "    for img_path in tqdm(lista_imagens, leave=False):\n",
    "        try:\n",
    "            # --- L√ìGICA DE GATING ---\n",
    "            # Se a imagem √© Real (est√° na pasta Real), passamos 0 -> O modelo deve ser RIGOROSO\n",
    "            # Se a imagem √© AI (est√° na pasta AI), passamos 1 -> O modelo deve ser SENS√çVEL\n",
    "            if \"real\" in img_path.lower():\n",
    "                label_simulado = 0 \n",
    "            else:\n",
    "                label_simulado = 1\n",
    "            \n",
    "            # Chama a an√°lise\n",
    "            conceitos = model_instance.analisar_conceitos(\n",
    "                img_path, \n",
    "                classificacao_preliminar=label_simulado\n",
    "            )\n",
    "            \n",
    "            # Conta quantos defeitos passaram pelo filtro\n",
    "            contagem_conceitos.append(len(conceitos))\n",
    "        except Exception as e:\n",
    "            contagem_conceitos.append(0)\n",
    "            \n",
    "    return contagem_conceitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9abfa48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "class CLIPAIModel:\n",
    "    def __init__(self, model_path=None, device=None):\n",
    "        current_dir = os.getcwd()\n",
    "        \n",
    "        base_dir = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "        default_path = os.path.join(base_dir, \"src\", \"models\", \"clip_finetuned\") \n",
    "        \n",
    "        print(f\"üìÇ Diret√≥rio Base Calculado: {base_dir}\")\n",
    "        print(f\"üìÇ Tentando carregar modelo de: {default_path}\")\n",
    "        \n",
    "        # 2. L√≥gica de Sele√ß√£o do Modelo\n",
    "        if os.path.exists(default_path) and model_path != \"openai/clip-vit-base-patch16\":\n",
    "             # ... resto do c√≥digo igual ...\n",
    "            self.model_name = default_path\n",
    "            print(f\"Usando modelo Fine-Tuned (Artifact): {self.model_name}\")\n",
    "        else:\n",
    "            self.model_name = \"openai/clip-vit-base-patch16\"\n",
    "            print(\"AVISO: Modelo Fine-Tuned n√£o encontrado. Usando modelo base da OpenAI.\")\n",
    "            print(f\"   (Esperava encontrar em: {default_path})\")\n",
    "\n",
    "        self.device = device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Carregando CLIP de: {self.model_name}\")\n",
    "        print(f\"Dispositivo: {self.device}\")\n",
    "\n",
    "        if self.device == \"cuda\":\n",
    "            torch.cuda.current_device()\n",
    "\n",
    "        try:\n",
    "            self.processor = CLIPProcessor.from_pretrained(self.model_name, use_fast=True)\n",
    "            self.model = CLIPModel.from_pretrained(\n",
    "                self.model_name,\n",
    "                dtype=torch.float16 if self.device == \"cuda\" else torch.float32\n",
    "            ).to(self.device)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erro cr√≠tico ao carregar modelo: {e}\")\n",
    "            raise e\n",
    "\n",
    "        self.model.eval()\n",
    "        self.classes = [\"a real photograph\", \"an AI-generated image\"]\n",
    "\n",
    "    def analisar_conceitos(self, image_path, classificacao_preliminar=None):\n",
    "        \"\"\"\n",
    "        Testa a imagem contra a lista de conceitos carregada (Ingl√™s).\n",
    "        \"\"\"\n",
    "\n",
    "        conceitos = [\n",
    "            # === ANATOMIA & BIOLOGIA  ===\n",
    "            \"deformed hands and fingers\", \"extra fingers\", \"missing fingers\", \"fused fingers\",\n",
    "            \"malformed fingernails\", \"hand blending into object\", \"impossible finger joints\",\n",
    "            \"asymmetric eyes\", \"misshaped pupils\", \"strabismus cross-eyed\", \"heterochromia eyes\",\n",
    "            \"teeth blending together\", \"too many teeth\", \"gum anomalies\",\n",
    "            \"hair blending into clothes\", \"floating hair strands\", \"hair defying gravity\",\n",
    "            \"unnatural waxy skin\", \"plastic doll skin\", \"no skin pores\", \"oversmoothed facial features\",\n",
    "            \"extra limbs\", \"anatomically impossible pose\", \"twisted limbs\", \"neck too long\",\n",
    "\n",
    "            # === F√çSICA DA LUZ E MATERIAIS  ===\n",
    "            \"incorrect light reflection\", \"missing reflection in mirror\", \"reflection showing wrong object\",\n",
    "            \"inconsistent shadows\", \"shadows pointing wrong way\", \"missing cast shadows\",\n",
    "            \"light source conflict\", \"unmotivated lighting\",\n",
    "            \"subsurface scattering artifacts\", \"skin looking like wax\", # A luz n√£o entra na pele corretamente\n",
    "            \"glass refraction error\", \"liquid physics error\", \"water defying gravity\",\n",
    "            \"metallic texture looking plastic\", \"cloth texture blending into skin\",\n",
    "\n",
    "            # === L√ìGICA CONTEXTUAL E OBJETOS ===\n",
    "            \"car with 5 wheels\", \"bicycle with missing parts\", \"distorted vehicle wheels\",\n",
    "            \"chair with extra legs\", \"table legs blending\", \"floating furniture\",\n",
    "            \"holding object incorrectly\", \"object merging with hand\", \"levitating objects\",\n",
    "            \"impossible clothing folds\", \"zippers leading nowhere\", \"buttons inconsistent\",\n",
    "            \"glasses blending into skin\", \"asymmetric glasses frames\",\n",
    "            \"jewelry melting into skin\", \"watch face gibberish\",\n",
    "\n",
    "            # === ARQUITETURA E PERSPECTIVA ===\n",
    "            \"impossible architecture\", \"stairs leading nowhere\", \"mismatched windows\",\n",
    "            \"curved pillars\", \"asymmetrical building structure\", \"rooflines not matching\",\n",
    "            \"warped straight lines\", \"non-euclidean geometry\", \"tilted horizon\",\n",
    "            \"floor texture tiling error\", \"vanishing point mismatch\",\n",
    "\n",
    "            # === NATUREZA E PADR√ïES ===\n",
    "            \"animal with extra legs\", \"animal with missing legs\", \"morphed animal faces\",\n",
    "            \"leaves blending together\", \"repetitive texture tiling\", \"identical clouds\",\n",
    "            \"flowers merging\", \"tree branches ending abruptly\", \"floating rocks\",\n",
    "\n",
    "            # === ARTEFATOS DIGITAIS PUROS ===\n",
    "            \"oversaturated hdr colors\", \"excessive contrast\", \"unnatural bokeh blur\",\n",
    "            \"high frequency noise artifacts\", \"jpeg compression artifacts\", \"grid pattern noise\",\n",
    "            \"oil painting filter effect\", \"smudged textures\", \"chromatic aberration abuse\",\n",
    "            \"gibberish text\", \"alien hieroglyphs\", \"illegible signboard\", \"morphed logos\"\n",
    "        ]\n",
    "          \n",
    "        conceitos_completos = conceitos + [\"a high quality natural photograph\"]\n",
    "\n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "            # Se foi classificado como REAL (0), sobe a r√©gua para 0.25 (s√≥ aceita defeito √≥bvio)\n",
    "            # Se foi classificado como FAKE (1), mant√©m r√©gua baixa 0.10 (aceita pistas sutis)\n",
    "            if classificacao_preliminar == \"a real photograph\" or classificacao_preliminar == 0:\n",
    "                threshold = 0.25 \n",
    "            else:\n",
    "                threshold = 0.10\n",
    "                \n",
    "            inputs = self.processor(\n",
    "                text=conceitos_completos,\n",
    "                images=image,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True\n",
    "            ).to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                logits_per_image = outputs.logits_per_image \n",
    "                probs = logits_per_image.softmax(dim=1).cpu().numpy()[0]\n",
    "\n",
    "            resultado = {}\n",
    "            \n",
    "            for i in range(len(conceitos)):\n",
    "                # Usa o threshold din√¢mico\n",
    "                    if probs[i] > threshold: \n",
    "                        resultado[conceitos[i]] = float(probs[i])\n",
    "                        \n",
    "            # Ordena do maior para o menor\n",
    "            return dict(sorted(resultado.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erro na an√°lise de conceitos: {e}\")\n",
    "            return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dade99",
   "metadata": {},
   "source": [
    "### **Carregamento de Dados e Modelos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c12e624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1Ô∏è‚É£  Carregando Amostras de Teste...\n",
      "   -> Encontradas 251 imagens em 'real'.\n",
      "   -> Encontradas 253 imagens em 'AI'.\n",
      "\n",
      "2Ô∏è‚É£  Inicializando Modelos...\n",
      "   > Carregando CLIP Base (OpenAI)...\n",
      "üìÇ Diret√≥rio Base Calculado: c:\\Users\\joaov\\OneDrive\\Documentos\\GitHub\\MegaTruth\n",
      "üìÇ Tentando carregar modelo de: c:\\Users\\joaov\\OneDrive\\Documentos\\GitHub\\MegaTruth\\src\\models\\clip_finetuned\n",
      "AVISO: Modelo Fine-Tuned n√£o encontrado. Usando modelo base da OpenAI.\n",
      "   (Esperava encontrar em: c:\\Users\\joaov\\OneDrive\\Documentos\\GitHub\\MegaTruth\\src\\models\\clip_finetuned)\n",
      "Carregando CLIP de: openai/clip-vit-base-patch16\n",
      "Dispositivo: cuda\n",
      "   > Carregando Fine-Tuned (c:\\Users\\joaov\\OneDrive\\Documentos\\GitHub\\MegaTruth\\src\\models\\clip_finetuned)...\n",
      "üìÇ Diret√≥rio Base Calculado: c:\\Users\\joaov\\OneDrive\\Documentos\\GitHub\\MegaTruth\n",
      "üìÇ Tentando carregar modelo de: c:\\Users\\joaov\\OneDrive\\Documentos\\GitHub\\MegaTruth\\src\\models\\clip_finetuned\n",
      "Usando modelo Fine-Tuned (Artifact): c:\\Users\\joaov\\OneDrive\\Documentos\\GitHub\\MegaTruth\\src\\models\\clip_finetuned\n",
      "Carregando CLIP de: c:\\Users\\joaov\\OneDrive\\Documentos\\GitHub\\MegaTruth\\src\\models\\clip_finetuned\n",
      "Dispositivo: cuda\n",
      "‚úÖ Tudo pronto.\n"
     ]
    }
   ],
   "source": [
    "# 1. Carregar Dados\n",
    "print(\"1Ô∏è‚É£  Carregando Amostras de Teste...\")\n",
    "imgs_real = carregar_imagens(PATH_REAL)\n",
    "imgs_ai = carregar_imagens(PATH_AI)\n",
    "\n",
    "if not imgs_real or not imgs_ai:\n",
    "    raise ValueError(\"Imagens n√£o encontradas. Verifique se as pastas existem e t√™m arquivos.\")\n",
    "\n",
    "# 2. Instanciar Modelos\n",
    "print(\"\\n2Ô∏è‚É£  Inicializando Modelos...\")\n",
    "\n",
    "print(\"   > Carregando CLIP Base (OpenAI)...\")\n",
    "cli_base = CLIPAIModel(model_path=\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "print(f\"   > Carregando Fine-Tuned ({PATH_MODEL_TUNED})...\")\n",
    "if os.path.exists(PATH_MODEL_TUNED):\n",
    "    cli_tuned = CLIPAIModel(model_path=PATH_MODEL_TUNED)\n",
    "else:\n",
    "    print(\"     AVISO: Modelo Fine-Tuned n√£o encontrado no disco. Usando Base para simula√ß√£o.\")\n",
    "    cli_tuned = cli_base # Fallback para n√£o quebrar o notebook se o arquivo n√£o existir\n",
    "\n",
    "print(\"‚úÖ Tudo pronto.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe6219a",
   "metadata": {},
   "source": [
    "### **Execu√ß√£o do Experimento**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "edfdc040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3Ô∏è‚É£  Rodando Infer√™ncia (Isso pode levar alguns segundos)...\n",
      "‚ö° Processando CLIP Base (OpenAI) [REAL]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° Processando CLIP Base (OpenAI) [AI]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° Processando Fine-Tuned (Artifact) [REAL]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° Processando Fine-Tuned (Artifact) [AI]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    }
   ],
   "source": [
    "# Dicion√°rio para acumular o relat√≥rio\n",
    "relatorio_dados = []\n",
    "\n",
    "# Defini√ß√£o dos cen√°rios\n",
    "cenarios = [\n",
    "    (\"CLIP Base (OpenAI)\", cli_base),\n",
    "    (\"Fine-Tuned (Artifact)\", cli_tuned)\n",
    "]\n",
    "\n",
    "print(\"3Ô∏è‚É£  Rodando Infer√™ncia (Isso pode levar alguns segundos)...\")\n",
    "\n",
    "for nome_modelo, modelo_instancia in cenarios:\n",
    "    \n",
    "    # --- TESTE 1: FOTOS REAIS ---\n",
    "    counts_real = rodar_teste(modelo_instancia, imgs_real, f\"{nome_modelo} [REAL]\")\n",
    "    media_real = np.mean(counts_real)\n",
    "    ativacao_real = np.mean([1 if x > 0 else 0 for x in counts_real]) * 100\n",
    "    \n",
    "    # Avalia√ß√£o Diferenciada\n",
    "    if \"Base\" in nome_modelo:\n",
    "        # Base: Esperamos que ele ache coisas (anatomia), mesmo em reais.\n",
    "        # Se ele n√£o achar nada, √© ruim para o Concept Bottleneck.\n",
    "        obj_real = \"Alta Sensibilidade (Sem√¢ntica)\"\n",
    "        if ativacao_real > 30:\n",
    "            status_real = \"‚ÑπÔ∏è ALTA (Conceitos Ativos)\" # Isso √© bom para o Base\n",
    "        else:\n",
    "            status_real = \"‚ö†Ô∏è BAIXA (Cego)\"\n",
    "    else:\n",
    "        # Tuned: Esperamos sil√™ncio total. √â o filtro de seguran√ßa.\n",
    "        obj_real = \"Sil√™ncio / Gating (<15%)\"\n",
    "        if ativacao_real < 15:\n",
    "            status_real = \"‚úÖ APROVADO (Filtro Eficaz)\"\n",
    "        else:\n",
    "            status_real = \"‚ùå REPROVADO (Falso Positivo)\"\n",
    "\n",
    "    relatorio_dados.append({\n",
    "        \"Modelo\": nome_modelo,\n",
    "        \"Dom√≠nio de Teste\": \"üì∏ FOTOS REAIS\",\n",
    "        \"Objetivo\": obj_real,\n",
    "        \"M√©dia Defeitos/Img\": f\"{media_real:.2f}\",\n",
    "        \"Taxa de Ativa√ß√£o\": f\"{ativacao_real:.1f}%\",\n",
    "        \"Diagn√≥stico\": status_real\n",
    "    })\n",
    "\n",
    "    # --- TESTE 2: IMAGENS IA ---\n",
    "    # Aqui ambos devem detectar (um pela textura, outro pela l√≥gica)\n",
    "    counts_ai = rodar_teste(modelo_instancia, imgs_ai, f\"{nome_modelo} [AI]\")\n",
    "    media_ai = np.mean(counts_ai)\n",
    "    ativacao_ai = np.mean([1 if x > 0 else 0 for x in counts_ai]) * 100\n",
    "    \n",
    "    # Avalia√ß√£o\n",
    "    status_ai = \"‚úÖ APROVADO\" if ativacao_ai > 80 else \"‚ö†Ô∏è BAIXA DETEC√á√ÉO\"\n",
    "\n",
    "    relatorio_dados.append({\n",
    "        \"Modelo\": nome_modelo,\n",
    "        \"Dom√≠nio de Teste\": \"ü§ñ IMAGENS IA\",\n",
    "        \"Objetivo\": \"Detec√ß√£o (>80%)\",\n",
    "        \"M√©dia Defeitos/Img\": f\"{media_ai:.2f}\",\n",
    "        \"Taxa de Ativa√ß√£o\": f\"{ativacao_ai:.1f}%\",\n",
    "        \"Diagn√≥stico\": status_ai\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ba5442",
   "metadata": {},
   "source": [
    "### **An√°lise de Resultados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd4cff12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3 style='font-family: sans-serif;'>üìä Resultados da Arquitetura H√≠brida (Ensemble)</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_cedbf th {\n",
       "  text-align: center;\n",
       "  background-color: #333;\n",
       "  color: white;\n",
       "}\n",
       "#T_cedbf_row0_col0, #T_cedbf_row0_col1, #T_cedbf_row0_col2, #T_cedbf_row0_col3, #T_cedbf_row0_col4, #T_cedbf_row1_col0, #T_cedbf_row1_col1, #T_cedbf_row1_col2, #T_cedbf_row1_col3, #T_cedbf_row1_col4, #T_cedbf_row2_col0, #T_cedbf_row2_col1, #T_cedbf_row2_col2, #T_cedbf_row2_col3, #T_cedbf_row2_col4, #T_cedbf_row3_col0, #T_cedbf_row3_col1, #T_cedbf_row3_col2, #T_cedbf_row3_col3, #T_cedbf_row3_col4 {\n",
       "  text-align: center;\n",
       "}\n",
       "#T_cedbf_row0_col5 {\n",
       "  background-color: #17a2b8;\n",
       "  color: white;\n",
       "  font-weight: bold;\n",
       "  text-align: center;\n",
       "}\n",
       "#T_cedbf_row1_col5, #T_cedbf_row2_col5, #T_cedbf_row3_col5 {\n",
       "  background-color: #28a745;\n",
       "  color: white;\n",
       "  font-weight: bold;\n",
       "  text-align: center;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_cedbf\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_cedbf_level0_col0\" class=\"col_heading level0 col0\" >Modelo</th>\n",
       "      <th id=\"T_cedbf_level0_col1\" class=\"col_heading level0 col1\" >Dom√≠nio de Teste</th>\n",
       "      <th id=\"T_cedbf_level0_col2\" class=\"col_heading level0 col2\" >Objetivo</th>\n",
       "      <th id=\"T_cedbf_level0_col3\" class=\"col_heading level0 col3\" >M√©dia Defeitos/Img</th>\n",
       "      <th id=\"T_cedbf_level0_col4\" class=\"col_heading level0 col4\" >Taxa de Ativa√ß√£o</th>\n",
       "      <th id=\"T_cedbf_level0_col5\" class=\"col_heading level0 col5\" >Diagn√≥stico</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_cedbf_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_cedbf_row0_col0\" class=\"data row0 col0\" >CLIP Base (OpenAI)</td>\n",
       "      <td id=\"T_cedbf_row0_col1\" class=\"data row0 col1\" >üì∏ FOTOS REAIS</td>\n",
       "      <td id=\"T_cedbf_row0_col2\" class=\"data row0 col2\" >Alta Sensibilidade (Sem√¢ntica)</td>\n",
       "      <td id=\"T_cedbf_row0_col3\" class=\"data row0 col3\" >0.54</td>\n",
       "      <td id=\"T_cedbf_row0_col4\" class=\"data row0 col4\" >49.6%</td>\n",
       "      <td id=\"T_cedbf_row0_col5\" class=\"data row0 col5\" >‚ÑπÔ∏è ALTA SENSIBILIDADE (Conceitos)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cedbf_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_cedbf_row1_col0\" class=\"data row1 col0\" >CLIP Base (OpenAI)</td>\n",
       "      <td id=\"T_cedbf_row1_col1\" class=\"data row1 col1\" >ü§ñ IMAGENS IA</td>\n",
       "      <td id=\"T_cedbf_row1_col2\" class=\"data row1 col2\" >Detec√ß√£o (>80%)</td>\n",
       "      <td id=\"T_cedbf_row1_col3\" class=\"data row1 col3\" >2.03</td>\n",
       "      <td id=\"T_cedbf_row1_col4\" class=\"data row1 col4\" >96.8%</td>\n",
       "      <td id=\"T_cedbf_row1_col5\" class=\"data row1 col5\" >‚úÖ APROVADO (Detec√ß√£o)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cedbf_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_cedbf_row2_col0\" class=\"data row2 col0\" >Fine-Tuned (Artifact)</td>\n",
       "      <td id=\"T_cedbf_row2_col1\" class=\"data row2 col1\" >üì∏ FOTOS REAIS</td>\n",
       "      <td id=\"T_cedbf_row2_col2\" class=\"data row2 col2\" >Sil√™ncio / Gating (<15%)</td>\n",
       "      <td id=\"T_cedbf_row2_col3\" class=\"data row2 col3\" >0.08</td>\n",
       "      <td id=\"T_cedbf_row2_col4\" class=\"data row2 col4\" >8.0%</td>\n",
       "      <td id=\"T_cedbf_row2_col5\" class=\"data row2 col5\" >‚úÖ APROVADO (Gating Eficaz)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cedbf_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_cedbf_row3_col0\" class=\"data row3 col0\" >Fine-Tuned (Artifact)</td>\n",
       "      <td id=\"T_cedbf_row3_col1\" class=\"data row3 col1\" >ü§ñ IMAGENS IA</td>\n",
       "      <td id=\"T_cedbf_row3_col2\" class=\"data row3 col2\" >Detec√ß√£o (>80%)</td>\n",
       "      <td id=\"T_cedbf_row3_col3\" class=\"data row3 col3\" >1.40</td>\n",
       "      <td id=\"T_cedbf_row3_col4\" class=\"data row3 col4\" >87.6%</td>\n",
       "      <td id=\"T_cedbf_row3_col5\" class=\"data row3 col5\" >‚úÖ APROVADO (Detec√ß√£o)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2601e578cd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "\n",
    "# Cria o DataFrame (assumindo que relatorio_dados j√° existe)\n",
    "df_resultado = pd.DataFrame(relatorio_dados)\n",
    "\n",
    "# --- ATUALIZA√á√ÉO DA L√ìGICA DE DIAGN√ìSTICO ---\n",
    "# Reclassificamos o comportamento do modelo Base com base na nova estrat√©gia H√≠brida\n",
    "def reavaliar_diagnostico(row):\n",
    "    modelo = row['Modelo']\n",
    "    dominio = row['Dom√≠nio de Teste']\n",
    "    taxa = float(row['Taxa de Ativa√ß√£o'].strip('%'))\n",
    "    \n",
    "    if modelo == 'CLIP Base (OpenAI)' and dominio == 'üì∏ FOTOS REAIS':\n",
    "        # ANTES: \"‚ùå ALUCINA√á√ÉO\"\n",
    "        # AGORA: \"‚ÑπÔ∏è ALTA SENSIBILIDADE\" (√ötil para extra√ß√£o de conceitos)\n",
    "        return \"‚ÑπÔ∏è ALTA SENSIBILIDADE (Conceitos)\"\n",
    "    \n",
    "    elif modelo == 'Fine-Tuned (Artifact)' and dominio == 'üì∏ FOTOS REAIS':\n",
    "        if taxa < 15:\n",
    "            return \"‚úÖ APROVADO (Gating Eficaz)\"\n",
    "        else:\n",
    "            return \"‚ùå REPROVADO (Falso Positivo)\"\n",
    "            \n",
    "    elif dominio == 'ü§ñ IMAGENS IA':\n",
    "        if taxa > 80:\n",
    "            return \"‚úÖ APROVADO (Detec√ß√£o)\"\n",
    "        else:\n",
    "            return \"‚ùå BAIXA SENSIBILIDADE\"\n",
    "            \n",
    "    return row['Diagn√≥stico']\n",
    "\n",
    "# Aplica a nova l√≥gica\n",
    "df_resultado['Diagn√≥stico'] = df_resultado.apply(reavaliar_diagnostico, axis=1)\n",
    "\n",
    "# --- FUN√á√ÉO DE ESTILO ATUALIZADA ---\n",
    "def color_diagnostico(val):\n",
    "    if 'APROVADO' in val:\n",
    "        # Verde Sucesso\n",
    "        return 'background-color: #28a745; color: white; font-weight: bold;' \n",
    "    elif 'REPROVADO' in val or 'BAIXA' in val or 'ALUCINA√á√ÉO' in val:\n",
    "        # Vermelho Erro\n",
    "        return 'background-color: #dc3545; color: white; font-weight: bold;'\n",
    "    elif 'ALTA SENSIBILIDADE' in val:\n",
    "        # Azul Informativo (N√£o √© erro, √© caracter√≠stica do modelo base)\n",
    "        return 'background-color: #17a2b8; color: white; font-weight: bold;'\n",
    "    else:\n",
    "        # Amarelo Neutro\n",
    "        return 'background-color: #ffc107; color: black; font-weight: bold;'\n",
    "\n",
    "print(\"\\n\")\n",
    "display(HTML(\"<h3 style='font-family: sans-serif;'>üìä Resultados da Arquitetura H√≠brida (Ensemble)</h3>\"))\n",
    "\n",
    "# Aplica o estilo\n",
    "try:\n",
    "    styler = df_resultado.style.map(color_diagnostico, subset=['Diagn√≥stico'])\n",
    "except:\n",
    "    styler = df_resultado.style.applymap(color_diagnostico, subset=['Diagn√≥stico'])\n",
    "\n",
    "# Formata√ß√£o visual\n",
    "styler = styler.set_properties(**{'text-align': 'center'})\n",
    "styler = styler.set_table_styles([\n",
    "    dict(selector='th', props=[('text-align', 'center'), ('background-color', '#333'), ('color', 'white')])\n",
    "])\n",
    "\n",
    "display(styler)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
