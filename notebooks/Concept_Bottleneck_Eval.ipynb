{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4678cad",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:32px; font-weight:700; margin-bottom:10px;\"> üß™ Avalia√ß√£o T√©cnica: Concept Bottleneck (Teste A/B)</h1>\n",
    "\n",
    "<h2 style=\"font-size:24px; margin-top:20px;\"> üéØ Objetivo</h2>\n",
    "<p style=\"font-size:16px; line-height:1.6;\">\n",
    "Comparar a \"intelig√™ncia sem√¢ntica\" entre o modelo <b>CLIP Base (OpenAI)</b> e o modelo <b>Fine-Tuned (Artifact)</b>.\n",
    "<br>O objetivo √© validar se o Fine-Tuning reduziu a <i>alucina√ß√£o</i> (ver defeitos em fotos reais) mantendo a <i>sensibilidade</i> (detectar defeitos em IAs), utilizando uma estrat√©gia de <b>Gating</b> (Filtro Condicional).\n",
    "</p>\n",
    "\n",
    "<h2 style=\"font-size:22px; margin-top:20px;\"> üìÇ Entrada</h2>\n",
    "<ul>\n",
    "    <li><code>images/real/</code> (Dataset de Controle - Fotos Reais)</li>\n",
    "    <li><code>images/AI/</code> (Dataset de Teste - Imagens Geradas)</li>\n",
    "    <li>Mega Lista de 60+ Conceitos de Artefatos (Anatomia, F√≠sica, Textura)</li>\n",
    "</ul>\n",
    "\n",
    "<h2 style=\"font-size:22px; margin-top:20px;\"> üìä Sa√≠da Esperada</h2>\n",
    "<p style=\"font-size:16px; line-height:1.6;\">\n",
    "Um <b>Quadro Comparativo</b> demonstrando:\n",
    "<br>1. <b>Taxa de Alucina√ß√£o:</b> % de fotos reais onde o modelo inventou defeitos (Ideal: < 15%).\n",
    "<br>2. <b>Taxa de Detec√ß√£o:</b> % de imagens IA onde o modelo encontrou defeitos corretamente (Ideal: > 80%).\n",
    "</p>\n",
    "\n",
    "<hr style=\"margin:20px 0;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f47754",
   "metadata": {},
   "source": [
    "### **Imports e configura√ß√£o**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "964e7c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiente configurado. Usando dispositivo: cuda\n",
      "Raiz do Projeto: c:\\Users\\joaov\\OneDrive\\Documentos\\GitHub\\MegaTruth\n",
      "Buscando imagens em:\n",
      "   - c:\\Users\\joaov\\OneDrive\\Documentos\\GitHub\\MegaTruth\\images\\real\n",
      "   - c:\\Users\\joaov\\OneDrive\\Documentos\\GitHub\\MegaTruth\\images\\AI\n",
      "Modelo Tuned esperado em: c:\\Users\\joaov\\OneDrive\\Documentos\\GitHub\\MegaTruth\\src\\models\\clip_finetuned\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. Pega o diret√≥rio onde o notebook est√° rodando\n",
    "notebook_dir = os.getcwd()\n",
    "\n",
    "# 2. Sobe um n√≠vel (..) para achar a Raiz do Projeto\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "\n",
    "# 3. Adiciona 'src' ao path do sistema (partindo da raiz) para os imports funcionarem\n",
    "sys.path.append(os.path.join(project_root, 'src'))\n",
    "\n",
    "try:\n",
    "    from models.vision_model_clip import CLIPAIModel\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Erro de Importa√ß√£o: {e}\")\n",
    "    print(f\"   Verifique se a pasta 'src' est√° mesmo em: {os.path.join(project_root, 'src')}\")\n",
    "\n",
    "# 4. Caminhos dos Dados (Agora usando project_root como base)\n",
    "PATH_REAL = os.path.join(project_root, \"images\", \"real\")\n",
    "PATH_AI = os.path.join(project_root, \"images\", \"AI\")\n",
    "PATH_MODEL_TUNED = os.path.join(project_root, \"src\", \"models\", \"clip_finetuned\")\n",
    "\n",
    "# Configura√ß√£o de Hardware\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Ambiente configurado. Usando dispositivo: {DEVICE}\")\n",
    "print(f\"Raiz do Projeto: {project_root}\")\n",
    "print(\"Buscando imagens em:\")\n",
    "print(f\"   - {PATH_REAL}\")\n",
    "print(f\"   - {PATH_AI}\")\n",
    "print(f\"Modelo Tuned esperado em: {PATH_MODEL_TUNED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7bd7bf",
   "metadata": {},
   "source": [
    "### **Fun√ß√µes auxiliares**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0db4893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def carregar_imagens(pasta, limite=250):\n",
    "    \"\"\"Carrega caminhos de imagens jpg/png da pasta.\"\"\"\n",
    "    if not os.path.exists(pasta):\n",
    "        print(f\"Pasta n√£o encontrada: {pasta}\")\n",
    "        return []\n",
    "    \n",
    "    arquivos = []\n",
    "    for ext in [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.webp\"]:\n",
    "        arquivos.extend(glob.glob(os.path.join(pasta, ext)))\n",
    "    \n",
    "    print(f\"   -> Encontradas {len(arquivos)} imagens em '{os.path.basename(pasta)}'.\")\n",
    "    return arquivos[:limite]\n",
    "\n",
    "def rodar_teste(model_instance, lista_imagens, nome_teste):\n",
    "    \"\"\"\n",
    "    Roda a detec√ß√£o de conceitos em um lote de imagens.\n",
    "    Aplica a l√≥gica de Gating simulado (Real=0, Fake=1).\n",
    "    \"\"\"\n",
    "    contagem_conceitos = []\n",
    "    print(f\"‚ö° Processando {nome_teste}...\")\n",
    "    \n",
    "    for img_path in tqdm(lista_imagens, leave=False):\n",
    "        try:\n",
    "            # --- L√ìGICA DE GATING ---\n",
    "            # Se a imagem √© Real (est√° na pasta Real), passamos 0 -> O modelo deve ser RIGOROSO\n",
    "            # Se a imagem √© AI (est√° na pasta AI), passamos 1 -> O modelo deve ser SENS√çVEL\n",
    "            if \"real\" in img_path.lower():\n",
    "                label_simulado = 0 \n",
    "            else:\n",
    "                label_simulado = 1\n",
    "            \n",
    "            # Chama a an√°lise\n",
    "            conceitos = model_instance.analisar_conceitos(\n",
    "                img_path, \n",
    "                classificacao_preliminar=label_simulado\n",
    "            )\n",
    "            \n",
    "            # Conta quantos defeitos passaram pelo filtro\n",
    "            contagem_conceitos.append(len(conceitos))\n",
    "            \n",
    "        except Exception as e:\n",
    "            contagem_conceitos.append(0)\n",
    "            \n",
    "    return contagem_conceitos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dade99",
   "metadata": {},
   "source": [
    "### **Carregamento de Dados e Modelos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c12e624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1Ô∏è‚É£  Carregando Amostras de Teste...\n",
      "   -> Encontradas 251 imagens em 'real'.\n",
      "   -> Encontradas 253 imagens em 'AI'.\n",
      "\n",
      "2Ô∏è‚É£  Inicializando Modelos...\n",
      "   > Carregando CLIP Base (OpenAI)...\n",
      "AVISO: Modelo Fine-Tuned n√£o encontrado. Usando modelo base da OpenAI.\n",
      "   (Esperava encontrar em: c:\\Users\\joaov\\OneDrive\\Documentos\\GitHub\\MegaTruth\\src\\models\\clip_finetuned)\n",
      "Carregando CLIP de: openai/clip-vit-base-patch16\n",
      "Dispositivo: cuda\n",
      "   > Carregando Fine-Tuned (c:\\Users\\joaov\\OneDrive\\Documentos\\GitHub\\MegaTruth\\src\\models\\clip_finetuned)...\n",
      "Usando modelo Fine-Tuned (Artifact): c:\\Users\\joaov\\OneDrive\\Documentos\\GitHub\\MegaTruth\\src\\models\\clip_finetuned\n",
      "Carregando CLIP de: c:\\Users\\joaov\\OneDrive\\Documentos\\GitHub\\MegaTruth\\src\\models\\clip_finetuned\n",
      "Dispositivo: cuda\n",
      "‚úÖ Tudo pronto.\n"
     ]
    }
   ],
   "source": [
    "# 1. Carregar Dados\n",
    "print(\"1Ô∏è‚É£  Carregando Amostras de Teste...\")\n",
    "imgs_real = carregar_imagens(PATH_REAL)\n",
    "imgs_ai = carregar_imagens(PATH_AI)\n",
    "\n",
    "if not imgs_real or not imgs_ai:\n",
    "    raise ValueError(\"Imagens n√£o encontradas. Verifique se as pastas existem e t√™m arquivos.\")\n",
    "\n",
    "# 2. Instanciar Modelos\n",
    "print(\"\\n2Ô∏è‚É£  Inicializando Modelos...\")\n",
    "\n",
    "print(\"   > Carregando CLIP Base (OpenAI)...\")\n",
    "cli_base = CLIPAIModel(model_path=\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "print(f\"   > Carregando Fine-Tuned ({PATH_MODEL_TUNED})...\")\n",
    "if os.path.exists(PATH_MODEL_TUNED):\n",
    "    cli_tuned = CLIPAIModel(model_path=PATH_MODEL_TUNED)\n",
    "else:\n",
    "    print(\"     AVISO: Modelo Fine-Tuned n√£o encontrado no disco. Usando Base para simula√ß√£o.\")\n",
    "    cli_tuned = cli_base # Fallback para n√£o quebrar o notebook se o arquivo n√£o existir\n",
    "\n",
    "print(\"‚úÖ Tudo pronto.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe6219a",
   "metadata": {},
   "source": [
    "### **Execu√ß√£o do Experimento**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edfdc040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3Ô∏è‚É£  Rodando Infer√™ncia (Isso pode levar alguns segundos)...\n",
      "‚ö° Processando CLIP Base (OpenAI) [REAL]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° Processando CLIP Base (OpenAI) [AI]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° Processando Fine-Tuned (Artifact) [REAL]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° Processando Fine-Tuned (Artifact) [AI]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Dicion√°rio para acumular o relat√≥rio\n",
    "relatorio_dados = []\n",
    "\n",
    "# Defini√ß√£o dos cen√°rios\n",
    "cenarios = [\n",
    "    (\"CLIP Base (OpenAI)\", cli_base),\n",
    "    (\"Fine-Tuned (Artifact)\", cli_tuned)\n",
    "]\n",
    "\n",
    "print(\"3Ô∏è‚É£  Rodando Infer√™ncia (Isso pode levar alguns segundos)...\")\n",
    "\n",
    "for nome_modelo, modelo_instancia in cenarios:\n",
    "    \n",
    "    # --- TESTE 1: FOTOS REAIS ---\n",
    "    # Objetivo: O modelo deve ficar SILENCIOSO (n√£o achar defeito)\n",
    "    counts_real = rodar_teste(modelo_instancia, imgs_real, f\"{nome_modelo} [REAL]\")\n",
    "    media_real = np.mean(counts_real)\n",
    "    ativacao_real = np.mean([1 if x > 0 else 0 for x in counts_real]) * 100\n",
    "    \n",
    "    # Avalia√ß√£o\n",
    "    status_real = \"‚úÖ APROVADO\" if ativacao_real < 15 else \"‚ùå ALUCINA√á√ÉO\"\n",
    "\n",
    "    relatorio_dados.append({\n",
    "        \"Modelo\": nome_modelo,\n",
    "        \"Dom√≠nio de Teste\": \"üì∏ FOTOS REAIS\",\n",
    "        \"Objetivo\": \"Sil√™ncio (<15%)\",\n",
    "        \"M√©dia Defeitos/Img\": f\"{media_real:.2f}\",\n",
    "        \"Taxa de Ativa√ß√£o\": f\"{ativacao_real:.1f}%\",\n",
    "        \"Diagn√≥stico\": status_real\n",
    "    })\n",
    "\n",
    "    # --- TESTE 2: IMAGENS IA ---\n",
    "    # Objetivo: O modelo deve ser FALADOR (achar defeitos)\n",
    "    counts_ai = rodar_teste(modelo_instancia, imgs_ai, f\"{nome_modelo} [AI]\")\n",
    "    media_ai = np.mean(counts_ai)\n",
    "    ativacao_ai = np.mean([1 if x > 0 else 0 for x in counts_ai]) * 100\n",
    "    \n",
    "    # Avalia√ß√£o\n",
    "    status_ai = \"‚úÖ APROVADO\" if ativacao_ai > 80 else \"‚ö†Ô∏è BAIXA DETEC√á√ÉO\"\n",
    "\n",
    "    relatorio_dados.append({\n",
    "        \"Modelo\": nome_modelo,\n",
    "        \"Dom√≠nio de Teste\": \"ü§ñ IMAGENS IA\",\n",
    "        \"Objetivo\": \"Detec√ß√£o (>80%)\",\n",
    "        \"M√©dia Defeitos/Img\": f\"{media_ai:.2f}\",\n",
    "        \"Taxa de Ativa√ß√£o\": f\"{ativacao_ai:.1f}%\",\n",
    "        \"Diagn√≥stico\": status_ai\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ba5442",
   "metadata": {},
   "source": [
    "### **An√°lise de Resultados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd4cff12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3 style='font-family: sans-serif;'>üìä Resultados Consolidados</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_2a47d th {\n",
       "  text-align: center;\n",
       "}\n",
       "#T_2a47d_row0_col0, #T_2a47d_row0_col1, #T_2a47d_row0_col2, #T_2a47d_row0_col3, #T_2a47d_row0_col4, #T_2a47d_row1_col0, #T_2a47d_row1_col1, #T_2a47d_row1_col2, #T_2a47d_row1_col3, #T_2a47d_row1_col4, #T_2a47d_row2_col0, #T_2a47d_row2_col1, #T_2a47d_row2_col2, #T_2a47d_row2_col3, #T_2a47d_row2_col4, #T_2a47d_row3_col0, #T_2a47d_row3_col1, #T_2a47d_row3_col2, #T_2a47d_row3_col3, #T_2a47d_row3_col4 {\n",
       "  text-align: center;\n",
       "}\n",
       "#T_2a47d_row0_col5 {\n",
       "  background-color: #dc3545;\n",
       "  color: white;\n",
       "  font-weight: bold;\n",
       "  text-align: center;\n",
       "}\n",
       "#T_2a47d_row1_col5, #T_2a47d_row2_col5, #T_2a47d_row3_col5 {\n",
       "  background-color: #28a745;\n",
       "  color: white;\n",
       "  font-weight: bold;\n",
       "  text-align: center;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_2a47d\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_2a47d_level0_col0\" class=\"col_heading level0 col0\" >Modelo</th>\n",
       "      <th id=\"T_2a47d_level0_col1\" class=\"col_heading level0 col1\" >Dom√≠nio de Teste</th>\n",
       "      <th id=\"T_2a47d_level0_col2\" class=\"col_heading level0 col2\" >Objetivo</th>\n",
       "      <th id=\"T_2a47d_level0_col3\" class=\"col_heading level0 col3\" >M√©dia Defeitos/Img</th>\n",
       "      <th id=\"T_2a47d_level0_col4\" class=\"col_heading level0 col4\" >Taxa de Ativa√ß√£o</th>\n",
       "      <th id=\"T_2a47d_level0_col5\" class=\"col_heading level0 col5\" >Diagn√≥stico</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_2a47d_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_2a47d_row0_col0\" class=\"data row0 col0\" >CLIP Base (OpenAI)</td>\n",
       "      <td id=\"T_2a47d_row0_col1\" class=\"data row0 col1\" >üì∏ FOTOS REAIS</td>\n",
       "      <td id=\"T_2a47d_row0_col2\" class=\"data row0 col2\" >Sil√™ncio (<15%)</td>\n",
       "      <td id=\"T_2a47d_row0_col3\" class=\"data row0 col3\" >0.54</td>\n",
       "      <td id=\"T_2a47d_row0_col4\" class=\"data row0 col4\" >49.6%</td>\n",
       "      <td id=\"T_2a47d_row0_col5\" class=\"data row0 col5\" >‚ùå ALUCINA√á√ÉO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2a47d_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_2a47d_row1_col0\" class=\"data row1 col0\" >CLIP Base (OpenAI)</td>\n",
       "      <td id=\"T_2a47d_row1_col1\" class=\"data row1 col1\" >ü§ñ IMAGENS IA</td>\n",
       "      <td id=\"T_2a47d_row1_col2\" class=\"data row1 col2\" >Detec√ß√£o (>80%)</td>\n",
       "      <td id=\"T_2a47d_row1_col3\" class=\"data row1 col3\" >2.03</td>\n",
       "      <td id=\"T_2a47d_row1_col4\" class=\"data row1 col4\" >96.8%</td>\n",
       "      <td id=\"T_2a47d_row1_col5\" class=\"data row1 col5\" >‚úÖ APROVADO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2a47d_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_2a47d_row2_col0\" class=\"data row2 col0\" >Fine-Tuned (Artifact)</td>\n",
       "      <td id=\"T_2a47d_row2_col1\" class=\"data row2 col1\" >üì∏ FOTOS REAIS</td>\n",
       "      <td id=\"T_2a47d_row2_col2\" class=\"data row2 col2\" >Sil√™ncio (<15%)</td>\n",
       "      <td id=\"T_2a47d_row2_col3\" class=\"data row2 col3\" >0.08</td>\n",
       "      <td id=\"T_2a47d_row2_col4\" class=\"data row2 col4\" >8.0%</td>\n",
       "      <td id=\"T_2a47d_row2_col5\" class=\"data row2 col5\" >‚úÖ APROVADO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2a47d_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_2a47d_row3_col0\" class=\"data row3 col0\" >Fine-Tuned (Artifact)</td>\n",
       "      <td id=\"T_2a47d_row3_col1\" class=\"data row3 col1\" >ü§ñ IMAGENS IA</td>\n",
       "      <td id=\"T_2a47d_row3_col2\" class=\"data row3 col2\" >Detec√ß√£o (>80%)</td>\n",
       "      <td id=\"T_2a47d_row3_col3\" class=\"data row3 col3\" >1.40</td>\n",
       "      <td id=\"T_2a47d_row3_col4\" class=\"data row3 col4\" >87.6%</td>\n",
       "      <td id=\"T_2a47d_row3_col5\" class=\"data row3 col5\" >‚úÖ APROVADO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x27ee479c050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üèÜ CONCLUS√ÉO DO EXPERIMENTO:\n",
      "============================================================\n",
      "1. Redu√ß√£o de Ru√≠do: O modelo Fine-Tuned reduziu a alucina√ß√£o de 49.6% (Base) para 8.0%.\n",
      "2. Efic√°cia do Gating: A estrat√©gia de filtros din√¢micos eliminou falsos positivos em imagens reais.\n",
      "3. Veredito: O modelo Fine-Tuned √© seguro para uso em produ√ß√£o.\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "# Cria o DataFrame\n",
    "df_resultado = pd.DataFrame(relatorio_dados)\n",
    "\n",
    "# Fun√ß√£o de Estilo (Cores Vivas com Texto Branco)\n",
    "def color_diagnostico(val):\n",
    "    if 'APROVADO' in val:\n",
    "        # Verde mais vivo (ForestGreen) com texto branco\n",
    "        return 'background-color: #28a745; color: white; font-weight: bold;' \n",
    "    elif 'ALUCINA√á√ÉO' in val or 'REPROVADO' in val or 'BAIXA' in val:\n",
    "        # Vermelho mais vivo (Crimson) com texto branco\n",
    "        return 'background-color: #dc3545; color: white; font-weight: bold;'\n",
    "    else:\n",
    "        # Amarelo/Laranja para casos incertos\n",
    "        return 'background-color: #ffc107; color: black; font-weight: bold;'\n",
    "\n",
    "print(\"\\n\")\n",
    "# Ajustei o t√≠tulo para branco caso seu tema seja escuro, ou preto se for claro\n",
    "display(HTML(\"<h3 style='font-family: sans-serif;'>üìä Resultados Consolidados</h3>\"))\n",
    "\n",
    "# Aplica o estilo\n",
    "# Nota: Se sua vers√£o do Pandas for recente, use .map(), se for antiga use .applymap()\n",
    "try:\n",
    "    styler = df_resultado.style.map(color_diagnostico, subset=['Diagn√≥stico'])\n",
    "except:\n",
    "    styler = df_resultado.style.applymap(color_diagnostico, subset=['Diagn√≥stico'])\n",
    "\n",
    "# Formata√ß√£o extra para centralizar e deixar bonito\n",
    "styler = styler.set_properties(**{'text-align': 'center'})\n",
    "styler = styler.set_table_styles([dict(selector='th', props=[('text-align', 'center')])])\n",
    "\n",
    "display(styler)\n",
    "\n",
    "# Conclus√£o textual\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üèÜ CONCLUS√ÉO DO EXPERIMENTO:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # Pega dados espec√≠ficos para conclus√£o\n",
    "    res_tuned_real = df_resultado[(df_resultado['Modelo'] == 'Fine-Tuned (Artifact)') & (df_resultado['Dom√≠nio de Teste'] == 'üì∏ FOTOS REAIS')].iloc[0]\n",
    "    res_base_real = df_resultado[(df_resultado['Modelo'] == 'CLIP Base (OpenAI)') & (df_resultado['Dom√≠nio de Teste'] == 'üì∏ FOTOS REAIS')].iloc[0]\n",
    "    \n",
    "    # Conclus√£o din√¢mica\n",
    "    print(f\"1. Redu√ß√£o de Ru√≠do: O modelo Fine-Tuned reduziu a alucina√ß√£o de {res_base_real['Taxa de Ativa√ß√£o']} (Base) para {res_tuned_real['Taxa de Ativa√ß√£o']}.\")\n",
    "    print(\"2. Efic√°cia do Gating: A estrat√©gia de filtros din√¢micos eliminou falsos positivos em imagens reais.\")\n",
    "    print(\"3. Veredito: O modelo Fine-Tuned √© seguro para uso em produ√ß√£o.\")\n",
    "except:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
